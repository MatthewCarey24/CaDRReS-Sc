{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Model\n",
    "#### This model will use NMF on the cell line x drug df to create latent vectors of each that will be used as features in a tensorflow predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer  # Import SimpleImputer for imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DF and create latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NDCG\n",
    "def ndcg_at_k(y_true, y_pred, k=10):\n",
    "    \"\"\"\n",
    "    Compute the Normalized Discounted Cumulative Gain (NDCG) at rank k.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True IC50 values (array-like)\n",
    "    - y_pred: Predicted IC50 values (array-like)\n",
    "    - k: Rank position for NDCG (default 10)\n",
    "    \n",
    "    Returns:\n",
    "    - NDCG score (float)\n",
    "    \"\"\"\n",
    "    # Sort the true values and predicted values in descending order\n",
    "    order_true = np.argsort(y_true)[::-1]\n",
    "    order_pred = np.argsort(y_pred)[::-1]\n",
    "    \n",
    "    # Compute DCG\n",
    "    dcg = 0\n",
    "    for i in range(k):\n",
    "        if i < len(y_true):\n",
    "            # Discounted Cumulative Gain (DCG)\n",
    "            dcg += (2**y_true[order_pred[i]] - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Compute Ideal DCG (IDCG)\n",
    "    idcg = 0\n",
    "    for i in range(k):\n",
    "        if i < len(order_true):\n",
    "            idcg += (2**y_true[order_true[i]] - 1) / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Latent Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path='../data/GDSC/gdsc_all_abs_ic50_bayesian_sigmoid_only9dosages.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "cell_line_obs_df = pd.read_csv(df_path, index_col=0)\n",
    "    \n",
    "# Impute missing values using the mean of each column (drug)\n",
    "imputer = SimpleImputer(strategy='mean')  # You can also try median or other strategies\n",
    "cell_line_obs_df_imputed = imputer.fit_transform(cell_line_obs_df)\n",
    "# Apply Truncated SVD for matrix factorization\n",
    "svd = TruncatedSVD(n_components=6)\n",
    "latent_matrix = svd.fit_transform(cell_line_obs_df_imputed)\n",
    "\n",
    "# Check if the imputed DataFrame has NaNs (should be no NaNs after imputation)\n",
    "assert not np.isnan(cell_line_obs_df_imputed).any(), \"Imputed DataFrame has NaN values!\"\n",
    "\n",
    "# Check if the latent matrix has NaNs (SVD should not produce NaNs if input is valid)\n",
    "assert not np.isnan(latent_matrix).any(), \"Latent matrix has NaN values!\"\n",
    "\n",
    "\n",
    "num_cell_lines = latent_matrix.shape[0]  # Rows of the latent matrix (cell lines)\n",
    "num_drugs = cell_line_obs_df.shape[1]    # Columns of the original matrix (drugs)\n",
    "\n",
    "num_cell_lines, num_drugs\n",
    "\n",
    "features = []\n",
    "targets = []\n",
    "\n",
    "# Loop over each cell line and drug combination\n",
    "for i in range(num_cell_lines):\n",
    "    for j in range(num_drugs):\n",
    "        # Get the latent vector for the cell line i\n",
    "        cell_line_latent = latent_matrix[i]    \n",
    "        # Get the latent vector for the drug j\n",
    "        drug_latent = svd.components_[:, j] \n",
    "        feature_vector = np.concatenate([cell_line_latent, drug_latent])\n",
    "        ic50_value = cell_line_obs_df_imputed[i, j]\n",
    "        features.append(feature_vector)\n",
    "        targets.append(ic50_value)\n",
    "\n",
    "# Convert the feature list and target list to numpy arrays\n",
    "features = np.array(features)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 160\n",
    "learning_rate = 0.0009647420370192299\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=frac_test, random_state=0)\n",
    "\n",
    "# Build the neural network model in TensorFlow\n",
    "model = keras.Sequential([ \n",
    "    layers.InputLayer(input_shape=(X_train.shape[1],)), \n",
    "    layers.Dense(hidden_units, activation='relu'), \n",
    "    layers.Dense(1)  # Single output unit for predicting IC50 value \n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and mean squared error loss\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='mse',  # Mean squared error for regression\n",
    "              metrics=['mae'])  # Mean absolute error for evaluation\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Evaluate the model on the validation set (R^2 score)\n",
    "y_pred = model.predict(X_test).flatten()  # Flatten to match dimensions\n",
    "\n",
    "# Calculate R^2\n",
    "r2_score = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "\n",
    "# Calculate NDCG at rank 10\n",
    "ndcg_score = ndcg_at_k(y_test, y_pred, k=10)\n",
    "\n",
    "print(f\"R^2 Score: {r2_score:.4f}, NDCG at 10: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to NMF (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m nmf \u001b[38;5;241m=\u001b[39m NMF(n_components\u001b[38;5;241m=\u001b[39mnmf_n_components, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m W, H \u001b[38;5;241m=\u001b[39m \u001b[43mnmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_line_obs_sparse\u001b[49m\u001b[43m)\u001b[49m, nmf\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\carey\\miniconda3\\envs\\cadrres_sc\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\carey\\miniconda3\\envs\\cadrres_sc\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\carey\\miniconda3\\envs\\cadrres_sc\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1655\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1650\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1651\u001b[0m     X, accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m   1652\u001b[0m )\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 1655\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruction_err_ \u001b[38;5;241m=\u001b[39m _beta_divergence(\n\u001b[0;32m   1658\u001b[0m     X, W, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss, square_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m )\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\carey\\miniconda3\\envs\\cadrres_sc\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1706\u001b[0m, in \u001b[0;36mNMF._fit_transform\u001b[1;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, update_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a NMF model for the data X and returns the transformed data.\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;124;03m        Actual number of iterations.\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1706\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNMF (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;66;03m# check parameters\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(X)\n",
      "File \u001b[1;32mc:\\Users\\carey\\miniconda3\\envs\\cadrres_sc\\lib\\site-packages\\sklearn\\utils\\validation.py:1689\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to NMF (input X)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "nmf_n_components = 50\n",
    "\n",
    "# Load the data\n",
    "cell_line_obs_df = pd.read_csv('../data/GDSC/gdsc_all_abs_ic50_bayesian_sigmoid_only9dosages.csv', index_col=0)\n",
    "\n",
    "# Convert the DataFrame to a sparse matrix format (CSR)\n",
    "cell_line_obs_sparse = sp.csr_matrix(cell_line_obs_df.fillna(0))  # Fill NaNs with 0s as a placeholder\n",
    "\n",
    "# Initialize NMF model\n",
    "nmf = NMF(n_components=nmf_n_components, init='random', random_state=0, max_iter=5000)\n",
    "\n",
    "# Fit the model\n",
    "W, H = nmf.fit_transform(cell_line_obs_sparse), nmf.components_.T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadrres_sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
